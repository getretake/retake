---
title: "On a Cloud Instance"
---

<Warning>
  Self-hosted cloud deployment is still under development. [Join the
  waitlist](https://retake.typeform.com/to/JTcYtLay) to be notified when it's
  production ready.
</Warning>

## Prerequisites

- The real-time server should be deployed on a computer with a minimum of 4GB memory, which is suitable for a development environment. 
  However, in a production setting, the configuration may differ and have greater resource requirements.
- The computer must expose a port for Kafka, by default port `9094` is used. For more information, refer to the
  `docker-compose.yaml` file for the recommended configuration.
- The computer must expose a port for Schema Registry, by default port `8081` is used. For more information, refer to the
  `docker-compose.yaml` file for the recommended configuration.
- Your source must configure logging for logical decoding. For example, Postgres need the `wal_level` set to `logical`.
  Refer to the [Setup](/quickstart/setup) page for configuration details.

### Instructions

1. SSH into your cloud instance.

2. Run the deploy script. It will use the `main` branch by default, but you
   can configure this with the `--branch` option. This will clone the repo,
   install the necessary dependencies, and spin up the real-time server inside a
   Docker container.

```bash
curl https://raw.githubusercontent.com/getretake/retake/main/deploy.sh | bash
```

3. Confirm the containers are running

```bash
CONTAINER ID   IMAGE                                     NAMES                      COMMAND                   CREATED         STATUS                            PORTS
                                 
828604a213d4   consumer                                  retake-consumer-1          "python -u connector…"    6 minutes ago   Up 6 minutes
0a38b8025d41   confluentinc/cp-kafka-connect-base:6.2.0  retake-connect-1           "bash -c 'echo \"Inst…"   6 minutes ago   Up 6 minutes (health: starting)   0.0.0.0:8083->8083/tcp, :::8083->8083/tcp, 9092/tcp   
7a3b655f74af   bitnami/schema-registry:7.2.5             retake-schema-registry-1   "/opt/bitnami/script…"    6 minutes ago   Up 6 minutes                      0.0.0.0:8081->8081/tcp, :::8081->8081/tcp             
187fa47011ae   bitnami/kafka:3.5                         retake-kafka-1             "/opt/bitnami/script…"    6 minutes ago   Up 6 minutes                      9092/tcp, 0.0.0.0:9094->9094/tcp, :::9094->9094/tcp   
```

4. Confirm the `retake-consumer` has successfully created the config topics. You can see this with `docker compose logs consumer`:

```bash
retake-consumer-1  | Topic _connector_config created
retake-consumer-1  | Topic _config_success created
```

5. Once everything is up and running, you can start using the sdk to create source and sink connectors, and listen to realtime changes.
   To start with creating connectors, create a `RealtimeServer` object:

  ```python
   my_rt_server = RealtimeServer(host="34.121.0.74", broker_port=9094, schema_registry_port=8081)
   ```

  <Note>
   The host argument must be the ip of your instance where the server was installed, with the broker and schema registry ports
    accessible by the sdk.
  </Note>


6. After creating your realtime server object in the sdk, run:

  ```python
  pipeline.create_real_time(my_rt_server)
  ```

  to create the connectors. This method will send the connection details of your source and sink to the broker and will automatically
  create and configure the connectors. 
  
    <Note>
    The [source](/concepts/source) must have an existing table configured with at least one record so
    the connector creates its topics correctly.
    </Note>

7. Now the realtime server is configured to automatically pull data from your source and push to the sink once processed. To start processing
  events, run:

  ```python
  pipeline.pipe_real_time(my_rt_server)
  ```

  The streams worker will start processing events using the transforms you defined in the pipeline, and it will automatically push the results
  to the sink you configured.